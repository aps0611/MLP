{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SVM: set of supervised learning methods used for `classification`, `regression`, and `outlier detection`"
      ],
      "metadata": {
        "id": "rPuMOBVsE3em"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM constructs hyperplane in a high plane or set of high plane in a high or infinite dimensional space, which can be used for classification regression or other tasks"
      ],
      "metadata": {
        "id": "cw4pcurEFDZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In sklearn we have three methods to implement SVM:\n",
        "\n",
        "1. SVC\n",
        "\n",
        "2. NuSVC\n",
        "\n",
        "3. LinearSVC - faster implementation of linear SVM and with only linear kernel- liblinear library based implementation"
      ],
      "metadata": {
        "id": "_NGcmGV0FU4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# how to implement SVC or C support classifier"
      ],
      "metadata": {
        "id": "HMCog_AzF4P6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Instantiate the SVC classifier estimator:\n",
        "```\n",
        "from sklearn.svm import SVC\n",
        "SVC_classifier = SVC()\n",
        "```\n",
        "\n",
        "Step 2: Call fit method with x_train and y_train viz label vector\n",
        "```\n",
        "SVC_classifier.fit(xtrain, ytrain)\n",
        "```"
      ],
      "metadata": {
        "id": "a_7rn4mkGauZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# how to perform Regularization on SVC:\n",
        "\n",
        "parameter `C`: Regularization parameter - float value\n",
        "\n",
        "Default C = 1.0"
      ],
      "metadata": {
        "id": "w_mHJ4HOGgBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "\n",
        "1. Strength of classifier is invesely proportional to C\n",
        "\n",
        "2. strictly positive\n",
        "\n",
        "3. penalty should be squared l2 penalty "
      ],
      "metadata": {
        "id": "fARsfnXvGt93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to specify kernel type"
      ],
      "metadata": {
        "id": "uLB8_lTAG4eM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `linear`\n",
        "2. `poly`\n",
        "3. `rbf`\n",
        "4. `sigmoid`\n",
        "5. `precomputed`"
      ],
      "metadata": {
        "id": "ALLzfe9KG8lO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "``` SVC_classifier = SVC(kernel = 'rbf') ```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ksNErkFTHIsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# how to set kernel coefficient for `rbf`, `poly` and `sigmoid`kernels?\n",
        "\n",
        "## `gamma` parameter\n",
        "\n",
        "1. `scale`: gamma = 1/(number of features)*(X.Var())\n",
        "2. `auto` : gamma = 1 / (number of features)\n",
        "3. `float value`: \n",
        "\n",
        "\n",
        "if kernel is `poly` or `sigmoid` then set `coef0` which is an independent term in kernel function."
      ],
      "metadata": {
        "id": "Ndk24JHtHZ1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "SVC_classifier = SVC()\n",
        "\n",
        "clf = SVC_classifier.fit(xtrain, ytrain)\n",
        "\n",
        "# to view the indices of support vectors\n",
        "clf.support_\n",
        "\n",
        "# to view the support vectors:\n",
        "clf.support_vectors_\n",
        "\n",
        "# to view the number of support vectors for each class:\n",
        "clf.n_support_\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "zmnh6X3IIufZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to implement NuSVC classifier estimator"
      ],
      "metadata": {
        "id": "Orxldf6EJR4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "\n",
        "from sklearn.svm import NuSVC\n",
        "NuSVC_class = NuSVC()\n",
        "\n",
        "NuSVC_class.fit(xtrain, ytrain)\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "YpvZhItTJVsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Signifinca of v in NuSVC\n",
        "\n",
        "instead of C in SVC there is `v` in NuSVC to control the number of support vectors and margin errors\n",
        "\n",
        "`v` is an `upper bound on the fraction of margin error` and `a lower bound of the fraction of support vectors`\n",
        "\n",
        "value of v c [0,1]\n",
        "\n",
        "default = 0.5"
      ],
      "metadata": {
        "id": "xOurBs3rJjJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LinearSVC"
      ],
      "metadata": {
        "id": "-4QtYq4EKCZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "from sklearn.svm import LinearSVC\n",
        "LinearSVC_classifier = LinearSVC()\n",
        "\n",
        "\n",
        "LinearSVC_classifier.fit(xtrain, ytrain)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "Kwlt4RYPKEiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to provide penalty:\n",
        "\n",
        "1. l1: leads to coef_ vectors that are sparse\n",
        "\n",
        "2. l2: \n",
        "\n",
        "default is l2"
      ],
      "metadata": {
        "id": "xY6Oi-mYKUeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to choose loss function in linear_SVC Classifier"
      ],
      "metadata": {
        "id": "fsMODYdjKjFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. hinge loss - stadard SVM\n",
        "\n",
        "2. squared_hinge - square of hinge loss\n",
        "\n",
        "\n",
        "default is squared_hinge loss"
      ],
      "metadata": {
        "id": "RCjrcECXKnNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters:\n",
        "\n",
        "1. `C`: regularization parameter\n",
        "\n",
        "2. `dual`: can solve either dual or primal optimization problem\n",
        "\n",
        "when `n_samples > n_features` prefer `dual = False`\n",
        "\n",
        "3. `fit_intercept`:"
      ],
      "metadata": {
        "id": "G2ZEEGI3K1XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to perform Multiclass classification using SVM:\n",
        "\n",
        "1. SVC and NuSVC implements one-versus-one approach for multiclass classification\n",
        "\n"
      ],
      "metadata": {
        "id": "NCOinppfLS4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`decision_function_shape`\n",
        "\n",
        "1. one_versus_one : ovo\n",
        "2. one_verus_rest : ovr\n",
        "\n",
        "linear_svc implement ovr approach\n"
      ],
      "metadata": {
        "id": "xatHggXcLmCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advantages of SVM:\n",
        "\n",
        "1. effective in high dimension spaces\n",
        "\n",
        "2. Features > Samples\n",
        "\n",
        "3. Uses a subset of training point in decision function called support vector\n",
        "\n",
        "4. Versatile: Different kernel functions can be specified for different decision function\n",
        "\n",
        "\n",
        "# Disadvantages;\n",
        "\n",
        "1. SVM do not directly provide probability estimates these are calcluated using expensive 5 fold cross validation\n",
        "\n",
        "2. avoid overfitting in choosing kernel functions if the number of featurs is much greater than number of smamples"
      ],
      "metadata": {
        "id": "dCQNPnzDL9ko"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LQXDzgE_GfbG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dKbTom78E0iX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LeiP5boFGFuW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}