{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpWBo7Y_Lucf"
      },
      "source": [
        "#Classification using Bagging and Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhMYg64GL0UM"
      },
      "source": [
        "In this colab, we will take Abalone data set and apply different techniques of bagging and boosting. In particular, we will apply the following techniques:\n",
        "\n",
        "**Bagging:**\n",
        "* `sklearn.ensemble.BaggingClassifier`\n",
        "* `sklearn.ensemble.RandomForestClassifier`\n",
        "\n",
        "**Boosting:**\n",
        "* `sklearn.ensemble.GradientBoostingClassifier`\n",
        "* `sklearn.ensemble.AdaBoostClassifier`\n",
        "\n",
        "We will also apply **VotingClassifier** that is implemented in sklearn as:\n",
        "\n",
        "`sklearn.ensemble.VotingClassifier`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOciokxxMNeK"
      },
      "source": [
        "Below, we provide details of most relevant parameters of the constructors of these classes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbdUBnk8LN6N"
      },
      "source": [
        "# `sklearn.ensemble.BaggingClassifier`\n",
        "\n",
        "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregates their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a base estimator (e.g., a decision tree).\n",
        "\n",
        "If the samples are drawn with replacement, then the method is known as **Bagging** wheras, if the samples are drawn without replacement, the method is known as **pasting**.\n",
        "\n",
        "Following are some useful parameters:\n",
        "\n",
        "* `base_estimator:` object, default=None\n",
        "\n",
        "The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a DecisionTreeClassifier.\n",
        "\n",
        "* `n_estimators:` int, default=10\n",
        "\n",
        "The number of base estimators in the ensemble.\n",
        "\n",
        "* `max_samples:` int or float, default=1.0\n",
        "\n",
        "The number of samples to draw from X to train each base estimator (with replacement by default) \n",
        "\n",
        "* `max_features:` int or float, default=1.0\n",
        "\n",
        "The number of features to draw from X to train each base estimator (without replacement by default)\n",
        "\n",
        "* `bootstrap:` bool, default=True\n",
        "\n",
        "Whether samples are drawn with replacement. If False, sampling without replacement is performed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I7141-FVBoWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdyhwdE-M5_7"
      },
      "source": [
        "# `sklearn.ensemble.RandomForestClassifier`\n",
        "\n",
        "Random forests differ from bagging by forcing the tree to use only a subset of its available features while constructing each tree. All the decision trees that make up a random forest are different because each tree is built on a different random subset of data\n",
        "\n",
        "Following are some useful parameters:\n",
        "* `n_estimators:` int, default=100\n",
        "\n",
        "The number of trees in the forest.\n",
        "\n",
        "* `criterion:` {“gini”, “entropy”}, default=”gini”\n",
        "\n",
        "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. \n",
        "\n",
        "* `max_depth:` int, default=None\n",
        "\n",
        "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "\n",
        "* `min_samples_split:` int or float, default=2\n",
        "\n",
        "The minimum number of samples required to split an internal node.\n",
        "\n",
        "* `min_samples_leaf:` int or float, default=1\n",
        "\n",
        "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. \n",
        "\n",
        "* `min_impurity_decrease:` float, default=0.0\n",
        "\n",
        "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
        "\n",
        "* `bootstrap:` bool, default=True\n",
        "\n",
        "Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySxKjB1uUPaT"
      },
      "source": [
        "#sklearn.ensemble.GradientBoostingClassifier\n",
        "\n",
        "In Gradient Boosting, each predictor tries to improve on its predecessor by reducing the errors. Instead of fitting a predictor on the data at each iteration, GradientBoostingClassifier fits a new predictor to the residual errors made by the previous predictor. \n",
        "\n",
        "Following are some useful parameters:\n",
        "\n",
        "* `n_estimators:` int, default=100\n",
        "\n",
        "The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n",
        "\n",
        "* `learning_rate:` float, default=0.1\n",
        "\n",
        "Learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.\n",
        "\n",
        "* `max_features:` int or float, default = None\n",
        "The number of features to consider when looking for the best split.\n",
        "\n",
        "* `max_depth:` int, default=3\n",
        "\n",
        "The maximum depth of the individual estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n",
        "\n",
        "* `min_impurity_decrease:` float, default=0.0\n",
        "\n",
        "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c91luYE_bjUE"
      },
      "source": [
        "#sklearn.ensemble.AdaBoostClassifier\n",
        "\n",
        "The basic concept behind Adaboost is to set the weights of classifiers and training the data sample in each iteration such that it ensures the accurate predictions of unusual observations. \n",
        "\n",
        "Following are some useful parameters:\n",
        "\n",
        "* `base_estimator:` object, default=None\n",
        "\n",
        "The base estimator from which the boosted ensemble is built. If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1.\n",
        "\n",
        "* `n_estimators:` int, default=50\n",
        "\n",
        "The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n",
        "\n",
        "* `learning_rate:` float, default=1.0\n",
        "\n",
        "Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. There is a trade-off between the learning_rate and n_estimators parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc_EWPy17QtP"
      },
      "source": [
        "#sklearn.ensemble.VotingClassifier\n",
        "\n",
        "A Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class.\n",
        "\n",
        "It aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. The idea is that instead of creating separate dedicated models and finding the accuracy for each of them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class.\n",
        "\n",
        "It is of two types:\n",
        "\n",
        "* **Hard Voting:** In hard voting, the predicted output class is a class with the highest majority of votes. Suppose five classifiers predicted the output class (A, B, B, A, B), so here the majority predicted B as output. Hence B will be the final prediction.\n",
        "\n",
        "* **Soft Voting:** In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for the classes A and B by the five predictors are (0.2, 0.8), (0.5, 0.5), (0.8, 0.2), (0.6, 0.4) and (0.3, 0.7) respectively. So the average for class A is 0.48 and B is 0.52, the winner is class B because it has more average probability.\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "* `estimators:` list of (str, estimator) tuples\n",
        "\n",
        "* `voting:` {‘hard’, ‘soft’}, default=’hard’\n",
        "If ‘hard’, uses predicted class labels for majority rule voting. Else if ‘soft’, predicts the class label based on the argmax of the sums of the predicted probabilities.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAzSlryUgpeE"
      },
      "source": [
        "Let us now apply these different models on Abalone data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai_Be5uCbjWz"
      },
      "source": [
        "#Loading the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_JtJ53qbjrr"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRJk413-bjua"
      },
      "source": [
        "column_names = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings']\n",
        "abalone_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data', header=None, names=column_names)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OykiNzdbxWE",
        "outputId": "e00b5a1d-a6de-4399-9689-6f3b780b7554"
      },
      "source": [
        "abalone_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4177, 9)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "g0y7P4eNb0lm",
        "outputId": "6115b921-5375-452e-e859-94e850fdd4f1"
      },
      "source": [
        "abalone_data.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Length</th>\n",
              "      <th>Diameter</th>\n",
              "      <th>Height</th>\n",
              "      <th>Whole weight</th>\n",
              "      <th>Shucked weight</th>\n",
              "      <th>Viscera weight</th>\n",
              "      <th>Shell weight</th>\n",
              "      <th>Rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.5140</td>\n",
              "      <td>0.2245</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>0.150</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.2255</td>\n",
              "      <td>0.0995</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>0.070</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.6770</td>\n",
              "      <td>0.2565</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.210</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5160</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.1140</td>\n",
              "      <td>0.155</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.055</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I</td>\n",
              "      <td>0.425</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.3515</td>\n",
              "      <td>0.1410</td>\n",
              "      <td>0.0775</td>\n",
              "      <td>0.120</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>F</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.415</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.7775</td>\n",
              "      <td>0.2370</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.330</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>F</td>\n",
              "      <td>0.545</td>\n",
              "      <td>0.425</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.7680</td>\n",
              "      <td>0.2940</td>\n",
              "      <td>0.1495</td>\n",
              "      <td>0.260</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>M</td>\n",
              "      <td>0.475</td>\n",
              "      <td>0.370</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5095</td>\n",
              "      <td>0.2165</td>\n",
              "      <td>0.1125</td>\n",
              "      <td>0.165</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>F</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.8945</td>\n",
              "      <td>0.3145</td>\n",
              "      <td>0.1510</td>\n",
              "      <td>0.320</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Sex  Length  Diameter  ...  Viscera weight  Shell weight  Rings\n",
              "0   M   0.455     0.365  ...          0.1010         0.150     15\n",
              "1   M   0.350     0.265  ...          0.0485         0.070      7\n",
              "2   F   0.530     0.420  ...          0.1415         0.210      9\n",
              "3   M   0.440     0.365  ...          0.1140         0.155     10\n",
              "4   I   0.330     0.255  ...          0.0395         0.055      7\n",
              "5   I   0.425     0.300  ...          0.0775         0.120      8\n",
              "6   F   0.530     0.415  ...          0.1415         0.330     20\n",
              "7   F   0.545     0.425  ...          0.1495         0.260     16\n",
              "8   M   0.475     0.370  ...          0.1125         0.165      9\n",
              "9   F   0.550     0.440  ...          0.1510         0.320     19\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoX8WkPkb_ba"
      },
      "source": [
        "X = abalone_data.iloc[:, :-1]\n",
        "y = abalone_data.iloc[:, -1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvDQJnz8b_d8"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDzqng7Ub_gk"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xMsVvhIb_jB"
      },
      "source": [
        "numeric_features = ['Length', 'Diameter',\t'Height',\t'Whole weight',\t'Shucked weight',\t'Viscera weight',\t'Shell weight']\n",
        "categorical_features = [\"Sex\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPYtu-succJM"
      },
      "source": [
        "numeric_transformer = Pipeline(\n",
        "    steps=[(\"imputer\", SimpleImputer(missing_values = 0, strategy=\"constant\", fill_value = 0.107996)), (\"scaler\", StandardScaler())]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85lprmncccLb"
      },
      "source": [
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgQcpNSmccOK"
      },
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRyZo5LYb_mD"
      },
      "source": [
        "# Append classifier to preprocessing pipeline.\n",
        "# Now we have a full prediction pipeline.\n",
        "clf = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", BaggingClassifier())]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2OlJNQsclLS",
        "outputId": "18cdba69-0796-4cd5-f604-760fd7431678"
      },
      "source": [
        "clf.fit(X_train, y_train)\n",
        "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model score: 0.224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwjyPfowy7An"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaOjV9NHy8Uu"
      },
      "source": [
        "#BaggingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJfFY2eWclN1",
        "outputId": "dc217342-3c5e-4027-c2b7-e234b7ecb67d"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "acc = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 10)\n",
        "print(type(acc))\n",
        "print('Accuracy of each fold ', list(acc*100))\n",
        "print(\"Accuracy: {:.2f} %\".format(acc.mean()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "Accuracy of each fold  [20.8955223880597, 24.251497005988025, 21.55688622754491, 20.059880239520957, 21.856287425149702, 23.353293413173652, 24.251497005988025, 27.245508982035926, 24.550898203592812, 20.35928143712575]\n",
            "Accuracy: 22.84 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVIu8BvGhLbe"
      },
      "source": [
        "X_train_new = preprocessor.fit_transform(X_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAD7l_2vclQS",
        "outputId": "5bbdad52-e183-4beb-ff4e-80a6b88c46a0"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "tuned_parameters = [{'n_estimators': [10, 50, 100, 500],\n",
        "                    'max_samples': [0.05, 0.1, 0.2, 0.5]\n",
        "                     }]\n",
        "scores = ['recall']\n",
        "for score in scores:\n",
        "    \n",
        "    print()\n",
        "    print(f\"Tuning hyperparameters for {score}\")\n",
        "    print()\n",
        "    \n",
        "    clf_CV = GridSearchCV(\n",
        "        BaggingClassifier(), tuned_parameters,\n",
        "        scoring = f'{score}_macro'\n",
        "    )\n",
        "    clf_CV.fit(X_train_new, y_train)\n",
        "    \n",
        "    print(\"Best parameters:\")\n",
        "    print()\n",
        "    print(clf_CV.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores:\")\n",
        "    means = clf_CV.cv_results_[\"mean_test_score\"]\n",
        "    stds = clf_CV.cv_results_[\"std_test_score\"]\n",
        "    for mean, std, params in zip(means, stds,\n",
        "                                 clf_CV.cv_results_['params']):\n",
        "        print(f\"{mean:0.3f} (+/-{std*2:0.03f}) for {params}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tuning hyperparameters for recall\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters:\n",
            "\n",
            "{'max_samples': 0.2, 'n_estimators': 500}\n",
            "\n",
            "Grid scores:\n",
            "0.135 (+/-0.014) for {'max_samples': 0.05, 'n_estimators': 10}\n",
            "0.143 (+/-0.019) for {'max_samples': 0.05, 'n_estimators': 50}\n",
            "0.138 (+/-0.016) for {'max_samples': 0.05, 'n_estimators': 100}\n",
            "0.141 (+/-0.019) for {'max_samples': 0.05, 'n_estimators': 500}\n",
            "0.122 (+/-0.028) for {'max_samples': 0.1, 'n_estimators': 10}\n",
            "0.135 (+/-0.016) for {'max_samples': 0.1, 'n_estimators': 50}\n",
            "0.138 (+/-0.008) for {'max_samples': 0.1, 'n_estimators': 100}\n",
            "0.143 (+/-0.009) for {'max_samples': 0.1, 'n_estimators': 500}\n",
            "0.129 (+/-0.023) for {'max_samples': 0.2, 'n_estimators': 10}\n",
            "0.142 (+/-0.016) for {'max_samples': 0.2, 'n_estimators': 50}\n",
            "0.149 (+/-0.023) for {'max_samples': 0.2, 'n_estimators': 100}\n",
            "0.149 (+/-0.025) for {'max_samples': 0.2, 'n_estimators': 500}\n",
            "0.138 (+/-0.019) for {'max_samples': 0.5, 'n_estimators': 10}\n",
            "0.142 (+/-0.025) for {'max_samples': 0.5, 'n_estimators': 50}\n",
            "0.145 (+/-0.023) for {'max_samples': 0.5, 'n_estimators': 100}\n",
            "0.148 (+/-0.029) for {'max_samples': 0.5, 'n_estimators': 500}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPWO6XMMclS9"
      },
      "source": [
        "# Append classifier to preprocessing pipeline.\n",
        "# Now we have a full prediction pipeline.\n",
        "clf2 = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", BaggingClassifier(max_samples =0.2, n_estimators = 500, random_state = 42))]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX_G38YwlqLQ",
        "outputId": "b4606fdf-19ef-4602-f23a-d0768e23674f"
      },
      "source": [
        "clf2.fit(X_train, y_train)\n",
        "print(\"model score: %.3f\" % clf2.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model score: 0.254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZkoFE6AyfCS"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbP3zo2snKbL"
      },
      "source": [
        "#RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3k1kpo2GlUD3",
        "outputId": "ec8c1afb-25c3-4644-debc-d60a790aae2c"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Append classifier to preprocessing pipeline.\n",
        "# Now we have a full prediction pipeline.\n",
        "clf_RFC = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier())]\n",
        ")\n",
        "\n",
        "clf_RFC.fit(X_train, y_train)\n",
        "print(\"model score: %.3f\" % clf_RFC.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model score: 0.236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIVbLqT-pKZ7"
      },
      "source": [
        "X_train_new = preprocessor.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EADzT42UmgmR"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "tuned_parameters = [{'n_estimators': [50, 100, 250, 500],\n",
        "                    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "                    'max_depth' : [4,5,6,7,8,9,10],\n",
        "                    'criterion' :['gini', 'entropy']\n",
        "                     }]\n",
        "scores = ['recall']\n",
        "for score in scores:\n",
        "    \n",
        "    print()\n",
        "    print(f\"Tuning hyperparameters for {score}\")\n",
        "    print()\n",
        "    \n",
        "    clf_RFC_CV = GridSearchCV(\n",
        "        RandomForestClassifier(), tuned_parameters,\n",
        "        scoring = f'{score}_macro'\n",
        "    )\n",
        "    clf_RFC_CV.fit(X_train_new, y_train)\n",
        "    \n",
        "    print(\"Best parameters:\")\n",
        "    print()\n",
        "    print(clf_RFC_CV.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores:\")\n",
        "    means = clf_RFC_CV.cv_results_[\"mean_test_score\"]\n",
        "    stds = clf_RFC_CV.cv_results_[\"std_test_score\"]\n",
        "    for mean, std, params in zip(means, stds,\n",
        "                                 clf_RFC_CV.cv_results_['params']):\n",
        "        print(f\"{mean:0.3f} (+/-{std*2:0.03f}) for {params}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wdr5PQwmgo8",
        "outputId": "1e1adecb-58ae-4b6a-be5b-2aff53cf1498"
      },
      "source": [
        "clf_RFC2 = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier(criterion= 'gini', max_depth= 8, max_features = 'sqrt', n_estimators = 200))]\n",
        ")\n",
        "\n",
        "clf_RFC2.fit(X_train, y_train)\n",
        "print(\"model score: %.3f\" % clf_RFC2.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model score: 0.266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upADHp5mp3rm"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Boosted Decision Trees or GBDT\n",
        "\n",
        "Decision Trees or DT's are one of the simplest yet powerful machine learning algorithms. However, they suffer from overfitting to the training data. By using meta learning techniques on Decision Trees, both testing accuracy as well as training time has improved substantially. While Bagging methods do yield significant improvement in the performance of Decision Trees, there are cases where Gradient Boosted Decision Trees ('GBDTs') using different boosting techniques achieve a better performance.\n",
        "\n",
        "We take a look at the following boosting techniques\n",
        "- Gradient Boosting in scikit-learn\n",
        "- Adaboost\n",
        "- XGBoost\n",
        "- Ligh GBM\n",
        "- CatBoost"
      ],
      "metadata": {
        "id": "usrG-2Ra36da"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Boosting in scikit-learn\n",
        "\n",
        "This is a generalization of boosting methods which can be used for both classification and regression. Let us understand the concept before implementing the module.\n",
        "\n",
        " At a high level, an ensemble of weak learners (whose prediction capability is slightly better than randomly guessing) is trained in a sequential manner. Every new iteration looks at the residual(prediction - ground truth) and tries to minimize the same. The outputs of the learners are combined in a weighted manner. The trajectory of the average loss function of such an ensemble follows a decreasing gradient path. It is also called as Forward Stagewise Additive Modeling\n"
      ],
      "metadata": {
        "id": "v__sCLl0yQHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Boosting - Algorithm\n",
        "\n",
        "Gradient boosting is also called as Forward Stagewise Additive Modeling.  The pseudo code is given below\n",
        "1. Initialize $f_o(x) = 0$\n",
        "2. For $m = 1$ to $M$\n",
        "- Compute\n",
        "$(\\beta_m,\\gamma_m) = \\underset{\\beta,\\gamma}{\\mathrm{argmin}}$ $\\sum_{i=1} ^{N} L(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma))$\n",
        "- Set $f_m(x) = f_{m-1}(x)+\\beta_mb(x;\\gamma_m)$\n",
        "\n",
        "where\n",
        "- $f_m()$ is the $m$th weak learner\n",
        "- $m = 1$ to $M$ is the number of steps\n",
        "- $L$ is  the loss function. The loss function is \n",
        "- $y_i$ is the true output\n",
        "- $\\beta_m$ is the step size or expanision co-efficients\n",
        "- $\\gamma$ is the set of parameters which in case of trees parameterizes the split variables and split points at the internal nodes and predictions at the terminal nodes.\n",
        "\n",
        "Squared-error loss in case of Gradient Boosting is given by\n",
        "\\begin{aligned}\n",
        "L(y,f(x)&= (y-f(x))^2\\\\\n",
        "or, L(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma)) &= (y_i - f_{m-1}(x_i)-\\beta b(x_i;\\gamma))^2\\\\\n",
        "&= (r_{im} -\\beta b(x_i;\\gamma))^2\n",
        "\\end{aligned}\n",
        "\n",
        "where $r_{im} = y_i - f_{m-1}(x_i)$ is the residual of the current model on the ith observation.\n",
        "\n",
        "Squared-error loss is not ideally suited for classification problem section and therefore, we use 'deviance' (also known as bimomial negative log-likelihood or cross entropy). The queation of deciance is \n",
        "$-l(Y,f(X)) = log(1+e^{-2Yf(x)})$\n"
      ],
      "metadata": {
        "id": "3BvkTqCZ6F6L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaCRk5Nep3ub"
      },
      "source": [
        "#GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UelDzKMjp7dw",
        "outputId": "542c3dd2-c9d5-4371-dd11-bfecea0e7991"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Append classifier to preprocessing pipeline.\n",
        "# Now we have a full prediction pipeline.\n",
        "clf_GBC = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", GradientBoostingClassifier())]\n",
        ")\n",
        "\n",
        "clf_GBC.fit(X_train, y_train)\n",
        "print(\"model score: %.3f\" % clf_GBC.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model score: 0.231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRch05zqp7gk"
      },
      "source": [
        "X_train_new = preprocessor.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTU0HTv9lUJL"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "tuned_parameters = [{\n",
        "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
        "    # \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
        "    # \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
        "    \"max_depth\":[3,5,8],\n",
        "    \"max_features\":[\"log2\",\"sqrt\"],\n",
        "    # \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
        "    \"n_estimators\":[10]\n",
        "}]\n",
        "\n",
        "\n",
        "scores = ['recall']\n",
        "for score in scores:\n",
        "    \n",
        "    print()\n",
        "    print(f\"Tuning hyperparameters for {score}\")\n",
        "    print()\n",
        "    \n",
        "    clf_GBC_CV = GridSearchCV(\n",
        "        GradientBoostingClassifier(), tuned_parameters,\n",
        "        scoring = f'{score}_macro'\n",
        "    )\n",
        "    clf_GBC_CV.fit(X_train_new, y_train)\n",
        "    \n",
        "    print(\"Best parameters:\")\n",
        "    print()\n",
        "    print(clf_GBC_CV.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores:\")\n",
        "    means = clf_GBC_CV.cv_results_[\"mean_test_score\"]\n",
        "    stds = clf_GBC_CV.cv_results_[\"std_test_score\"]\n",
        "    for mean, std, params in zip(means, stds,\n",
        "                                 clf_GBC_CV.cv_results_['params']):\n",
        "        print(f\"{mean:0.3f} (+/-{std*2:0.03f}) for {params}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdcxQlo3coCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "165c0d3d-5b75-43b8-8975-cb0330e48f38"
      },
      "source": [
        "clf_GBC2 = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", GradientBoostingClassifier(learning_rate= 0.075, max_depth= 3, max_features='log2', n_estimators=10))]\n",
        ")\n",
        "\n",
        "clf_GBC2.fit(X_train, y_train)\n",
        "print(\"model score: %.3f\" % clf_GBC2.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model score: 0.275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtzYjtUnyjUo"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYsY-ey-tj5q"
      },
      "source": [
        "#AdaBoostClassifier - Algorithm\n",
        "\n",
        "AdaBoost was invented by Robert Schapire who is Parner Reasearcher at Microsoft Research.\n",
        "\n",
        "AdaBoost Classifier works exactly as GBDT defined earlier. However, instead of using deviance, AdaBoost uses exponential loss\n",
        "\n",
        "The steps involved are\n",
        "- Assign weights to each of the sample/training data points as $w_i$. For the first learner model, assign the weights equally as $w_i = 1/N$ for $i =  1,2,...,N$\n",
        "- Repeat until $M$ iterations i.e. for $t= 1$ to $M$:\n",
        "  - Train a learner model $C_t(x)$ on the weighted training data points\n",
        "  - Compute the prediction error i.e.\n",
        "  $err_m$ = $\\frac{1}{\\sum_{i=1} ^{N} w_i}\\sum_{i=1} ^{N} w_iI(y_i\\neq C_m(x_i)$)\n",
        "  - Compute the step-size or weight of the respective learner model i.e.\n",
        "  $\\alpha_m = log((1-err_m)/err_m)$ \n",
        "  - Recalculate weight of the observations as $w_i^m = w_i^{m-1}*exp[\\alpha_m.I(y_i\\neq C_m(x_i)]$\n",
        "- Ouput Final Learner model as sum of all the models till the $Mth$ iteration i.e. $C(x) = \\sum_{t=1} ^{M} \\alpha_m.C_m(x_i)$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AdaBoost - Modus Operandi\n",
        "*source*\n",
        "\n",
        "*- An Introduction to Machine Learning by Prof. Balaraman Ravindran*\n",
        "\n",
        "*- The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani and Jerome Friedman*\n",
        "\n",
        "We look at the intuition behind AdaBoost. As we already know, AdaBoost aims to train individual classifiers $C_m(x) \\in \\{-1,1\\}$ in such a fashion so that exponential loss is minimized. Therefore,\n",
        "\n",
        "$(\\beta_m,C_m) = \\underset{\\beta,C}{\\mathrm{argmin}}$ $\\sum_{i=1} ^{N} exp(-y_i(f_{m-1}(x_i)+\\beta C(x_i))$\n",
        "\n",
        "Now,\n",
        "$w_i^{m} = exp(-y_i .f_{m-1}(x_i))$. Therefore, we can rewrite the above loss function as \n",
        "$(\\beta_m,C_m) = \\underset{\\beta,C}{\\mathrm{argmin}}$ $\\sum_{i=1} ^{N} w_i^{m} . exp(-y_i \\beta C(x_i))$\n",
        "\n",
        "Now, the above equation can be re-written as \n",
        "\n",
        "$e^{-\\beta} \\sum\\limits^{}_{y_1 = G(x_1)} w_i^{(m)} + e^{-\\beta} \\sum\\limits^{}_{y_1 \\neq G(x_1)} w_i^{(m)}$\n",
        "\n",
        "Basically, we can ee that there are two bins in which one bin contains correctly classified points and another contains incorrectly classified points. The goal of AdaBoost or for that matter any boosting technique is to get more points move from the incorrectly classified bin to the correctly classified bin."
      ],
      "metadata": {
        "id": "4J_iWve_yvwZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZApZWygathdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d73e44-6e7f-4deb-f7df-109616d0a24e"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Append classifier to preprocessing pipeline.\n",
        "# Now we have a full prediction pipeline.\n",
        "clf_ABC = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", AdaBoostClassifier())]\n",
        ")\n",
        "\n",
        "clf_ABC.fit(X_train, y_train)\n",
        "print(\"model score: %.3f\" % clf_ABC.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model score: 0.194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkwiollRthgE"
      },
      "source": [
        "X_train_new = preprocessor.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88YMeFeAthjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df4da9c-c687-47e4-88d6-a967acfd0e75"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "tuned_parameters = [{\n",
        "    'n_estimators': [1,50,100,150],    \n",
        "    'learning_rate': [0.1,0.4,0.7,1]\n",
        "}]\n",
        "\n",
        "\n",
        "scores = ['recall']\n",
        "for score in scores:\n",
        "    \n",
        "    print()\n",
        "    print(f\"Tuning hyperparameters for {score}\")\n",
        "    print()\n",
        "    \n",
        "    clf_ABC_CV = GridSearchCV(\n",
        "        AdaBoostClassifier(), tuned_parameters,\n",
        "        scoring = f'{score}_macro'\n",
        "    )\n",
        "    clf_ABC_CV.fit(X_train_new, y_train)\n",
        "    \n",
        "    print(\"Best parameters:\")\n",
        "    print()\n",
        "    print(clf_ABC_CV.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores:\")\n",
        "    means = clf_ABC_CV.cv_results_[\"mean_test_score\"]\n",
        "    stds = clf_ABC_CV.cv_results_[\"std_test_score\"]\n",
        "    for mean, std, params in zip(means, stds,\n",
        "                                 clf_ABC_CV.cv_results_['params']):\n",
        "        print(f\"{mean:0.3f} (+/-{std*2:0.03f}) for {params}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tuning hyperparameters for recall\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters:\n",
            "\n",
            "{'learning_rate': 0.4, 'n_estimators': 50}\n",
            "\n",
            "Grid scores:\n",
            "0.073 (+/-0.009) for {'learning_rate': 0.1, 'n_estimators': 1}\n",
            "0.070 (+/-0.006) for {'learning_rate': 0.1, 'n_estimators': 50}\n",
            "0.092 (+/-0.011) for {'learning_rate': 0.1, 'n_estimators': 100}\n",
            "0.094 (+/-0.013) for {'learning_rate': 0.1, 'n_estimators': 150}\n",
            "0.073 (+/-0.009) for {'learning_rate': 0.4, 'n_estimators': 1}\n",
            "0.100 (+/-0.014) for {'learning_rate': 0.4, 'n_estimators': 50}\n",
            "0.097 (+/-0.023) for {'learning_rate': 0.4, 'n_estimators': 100}\n",
            "0.098 (+/-0.019) for {'learning_rate': 0.4, 'n_estimators': 150}\n",
            "0.073 (+/-0.009) for {'learning_rate': 0.7, 'n_estimators': 1}\n",
            "0.093 (+/-0.009) for {'learning_rate': 0.7, 'n_estimators': 50}\n",
            "0.091 (+/-0.006) for {'learning_rate': 0.7, 'n_estimators': 100}\n",
            "0.091 (+/-0.007) for {'learning_rate': 0.7, 'n_estimators': 150}\n",
            "0.073 (+/-0.009) for {'learning_rate': 1, 'n_estimators': 1}\n",
            "0.081 (+/-0.012) for {'learning_rate': 1, 'n_estimators': 50}\n",
            "0.081 (+/-0.012) for {'learning_rate': 1, 'n_estimators': 100}\n",
            "0.081 (+/-0.012) for {'learning_rate': 1, 'n_estimators': 150}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz9NKLpwthmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aca1ae6c-164f-4496-90e7-d9898a98cfd3"
      },
      "source": [
        "clf_ABC2 = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", AdaBoostClassifier(learning_rate= 0.4, n_estimators= 50))]\n",
        ")\n",
        "\n",
        "clf_ABC2.fit(X_train, y_train)\n",
        "print(\"model score: %.3f\" % clf_ABC2.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model score: 0.246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhkLlGOJmYMc"
      },
      "source": [
        "#VotingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ac8vNQBoijs",
        "outputId": "b7f66fa5-fd24-415b-dc37-d04fd3690c4f"
      },
      "source": [
        "models = list()\n",
        "models.append(('knn1', KNeighborsClassifier(n_neighbors=1)))\n",
        "models.append(('knn3', KNeighborsClassifier(n_neighbors=3)))\n",
        "models.append(('knn5', KNeighborsClassifier(n_neighbors=5)))\n",
        "models.append(('knn7', KNeighborsClassifier(n_neighbors=7)))\n",
        "models.append(('knn9', KNeighborsClassifier(n_neighbors=9)))\n",
        "\n",
        "\n",
        "clf_VC = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", VotingClassifier(estimators=models,voting='hard'))]\n",
        ")\n",
        "\n",
        "clf_VC.fit(X_train, y_train)\n",
        "print(\"model score: %.3f\" % clf_VC.score(X_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model score: 0.206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTnSghVvo43i",
        "outputId": "72dbf7ae-464b-4475-b612-56e827ed3fd4"
      },
      "source": [
        "models = list()\n",
        "models.append(('svm1', SVC(probability=True, kernel='poly', degree=1)))\n",
        "models.append(('svm2', SVC(probability=True, kernel='poly', degree=2)))\n",
        "models.append(('svm3', SVC(probability=True, kernel='poly', degree=3)))\n",
        "models.append(('svm4', SVC(probability=True, kernel='poly', degree=4)))\n",
        "models.append(('svm5', SVC(probability=True, kernel='poly', degree=5)))\n",
        "\n",
        "\n",
        "clf_VC = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", VotingClassifier(estimators=models,voting='hard'))]\n",
        ")\n",
        "\n",
        "clf_VC.fit(X_train, y_train)\n",
        "print(\"model score: %.3f\" % clf_VC.score(X_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model score: 0.257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b29e9KU_pNPe",
        "outputId": "519e488d-2845-4f08-f7b6-2d3dd2777587"
      },
      "source": [
        "models = list()\n",
        "models.append(('cart1', DecisionTreeClassifier(max_depth=1)))\n",
        "models.append(('cart2', DecisionTreeClassifier(max_depth=2)))\n",
        "models.append(('cart3', DecisionTreeClassifier(max_depth=3)))\n",
        "models.append(('cart4', DecisionTreeClassifier(max_depth=4)))\n",
        "models.append(('cart5', DecisionTreeClassifier(max_depth=5)))\n",
        "\n",
        "clf_VC = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", VotingClassifier(estimators=models,voting='hard'))]\n",
        ")\n",
        "\n",
        "clf_VC.fit(X_train, y_train)\n",
        "print(\"model score: %.3f\" % clf_VC.score(X_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model score: 0.269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71pxFMCxp1EU",
        "outputId": "d9180b68-8fc5-4515-f646-94165a4ddd20"
      },
      "source": [
        "models = list()\n",
        "models.append(('lr1', LogisticRegression(penalty = 'l1', solver='liblinear')))\n",
        "models.append(('lr2', LogisticRegression(penalty = 'l2', solver='liblinear')))\n",
        "models.append(('lr3', LogisticRegression(penalty = 'elasticnet', solver='saga', l1_ratio=0.5)))\n",
        "models.append(('lr4', LogisticRegression(penalty = 'none', solver='saga')))\n",
        "\n",
        "clf_VC = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", VotingClassifier(estimators=models,voting='hard'))]\n",
        ")\n",
        "\n",
        "clf_VC.fit(X_train, y_train)\n",
        "print(\"model score: %.3f\" % clf_VC.score(X_test, y_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model score: 0.273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwoeJjmGmYnq",
        "outputId": "8341559a-555f-4bb8-a108-ab3c80b0b375"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "lr = LogisticRegression()\n",
        "dt = DecisionTreeClassifier()\n",
        "svm= SVC(probability=True)\n",
        "knn= KNeighborsClassifier()\n",
        "\n",
        "# Append classifier to preprocessing pipeline.\n",
        "# Now we have a full prediction pipeline.\n",
        "clf_VC = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", VotingClassifier(estimators=[('lr', lr), ('dt', dt), ('svc', svm), ('knn',knn)],voting='hard'))]\n",
        ")\n",
        "\n",
        "clf_VC.fit(X_train, y_train)\n",
        "print(\"model score: %.3f\" % clf_VC.score(X_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model score: 0.269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Aju9j5xmZ1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nlI2Vwu-piDE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}